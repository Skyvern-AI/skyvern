{"id": "s-j4ju", "uuid": "340b6315-ae19-4604-83f4-8d5a0ca2d740", "title": "Block-Level Cache Key Templates", "file_path": "specs/block_level_cache_key_templates.md", "content": "## Overview\n\nIntroduce optional cache key templating at the workflow block level so that cached code/artifacts can vary per block instance using workflow parameters and prior block outputs. This augments the existing workflow-level cache key without breaking existing scripts.\n\n## Goals\n\n- Allow each block definition to specify an optional `cache_key_template` string that resolves with workflow/block context.\n- Ensure script generation, execution, persistence, and regeneration use the resolved block cache key so multiple variants can coexist.\n- Expose the setting in the workflow editor UI alongside existing cache controls.\n\n## Non-Goals\n\n- Changing how workflow-level cache keys are defined.\n- Modifying database schema beyond adding the new field to serialized models.\n- Redesigning broader caching invalidation policies.\n\n## Requirements\n\n### Backend API & Models\n\n- Extend all block models (`skyvern/forge/sdk/workflow/models/block.py`) to include `cache_key_template: str | None = None`, mirroring `disable_cache`.\n- Surface the field through:\n  - YAML/REST schemas (`skyvern/schemas/workflows.py` and fern/OpenAPI definitions).\n  - Generated client types (Python, TypeScript) and any DTO transforms.\n  - Codegen inputs (`transform_workflow_run_to_code_gen_input`) so regenerated scripts keep templates.\n- Rendering rules for an executing block:\n  1. Render the workflow-level cache key (if present) using existing logic; otherwise fall back to `workflow.workflow_permanent_id`.\n  1. If the block has a `cache_key_template`, render it with a context containing:\n    - Workflow parameters (same as current workflow-level rendering).\n    - `WorkflowRunContext` block metadata and recorded outputs for completed blocks.\n  1. If evaluation succeeds and yields a non-empty string, set `block_cache_key = f\"{base_key}:{rendered_block_key}\"`.\n  1. If rendering fails or produces an empty string, log a warning and fall back to `cache_key or label` (current behaviour) to avoid breaking execution.\n- Update utilities (`_render_template_with_label` and helpers) to provide the enriched context and return both rendered string and context for reuse.\n- Script generation (`generate_script.py`) must emit decorators using the resolved block cache key so cached functions are registered under the new value.\n- Execution helpers in `script_service.py` should request cached functions with the resolved key and store workflow run blocks using that key when creating/updating cached script mappings.\n- Adjust regeneration logic (`_regenerate_script_block_after_ai_fallback`) and `workflow_script_service` retrieval so cached scripts are looked up by the new block key, ensuring per-block variants coexist.\n\n### Frontend\n\n- Add a text input for \"Cache Key Template\" to block configuration panels next to \"Disable Cache\" for all block types that support caching.\n- Display helper text indicating templates may reference workflow parameters and previous block outputs.\n- Persist the new field through editor state → API payload → save operations.\n- Optionally provide basic validation (e.g., warn if unmatched `{`/`}`) but allow submission to keep backend authoritative.\n\n### Compatibility & Validation\n\n- Because the new field defaults to `None`, existing workflows/scripts remain valid without migration; ensure serializers handle missing data.\n- Regenerate API clients / fern SDKs so downstream code sees the new property.\n- Validate key length before use; if longer than backend/storage limits (to confirm), consider hashing or truncating with warning.\n- Ensure log messages avoid leaking sensitive template data when render failures occur.\n\n## Testing\n\n- Unit tests for template rendering covering workflow params, block outputs, fallback cases, and error handling.\n- Integration test: create workflow with multiple blocks using distinct cache key templates, run twice with different parameter values, verify separate cached scripts.\n- Frontend e2e/regression to confirm template field persists and renders in editor.\n\n## Open Questions\n\n1. Should we hash or otherwise normalize extremely long rendered keys rather than storing raw strings? (Investigate current limits.)\n1. Do we need admin tooling to list block-level cache key variants similar to existing workflow cache key views?\n1. Are there security/data-governance concerns with exposing prior block outputs to templates (e.g., secrets)? Document and mitigate if needed.", "priority": 2, "archived": 0, "archived_at": null, "created_at": "2025-11-06 19:30:22", "updated_at": "2025-11-11 05:12:12", "parent_id": null, "parent_uuid": null, "relationships": [], "tags": []}
{"id": "s-4oug", "uuid": "77ff88c4-3144-40ab-98fa-f05ef87fca43", "title": "Conditional Workflow Branching", "file_path": "specs/conditional_workflow_branching.md", "content": "## Overview\n\nSkyvern workflows currently execute blocks strictly in the order they are defined (`WorkflowDefinition.blocks`). Even though nested `ForLoopBlock`s can execute sublists of blocks, there is no way to branch execution based on runtime data—the workflow engine (`skyvern/forge/sdk/workflow/service.py`) simply iterates a list. This spec introduces a `ConditionalBlock` that can evaluate predicates against the `WorkflowRunContext` and route execution into different block subgraphs, establishing the first concrete step toward a DAG-based workflow engine.\n\n## Goals\n\n- Provide a first-class `ConditionalBlock` that supports multiple ordered branches (if/elif/else semantics) and executes the matching branch’s blocks.\n- Build a reusable graph view of workflow definitions so future blocks can have multiple successors without rewriting the executor again.\n- Persist, surface, and observe which branch was taken (predicate, evaluated result, outputs) so runs are explainable.\n- Update backend schemas, workflow execution, script generation, and clients to understand the new block type.\n- Ship an editor experience in `skyvern-frontend` that lets users configure conditions and arrange branch-specific blocks without breaking existing workflows.\n\n## Non-Goals\n\n- Parallel block execution or speculative evaluation of multiple branches.\n- Arbitrary DAG editing UI (dragging edges between any nodes); this spec only unlocks branching via the conditional block container.\n- Changing how block cache keys, retries, or action history work beyond what conditional branching requires.\n\n## Background\n\n- Workflow definitions (`skyvern/forge/sdk/workflow/models/workflow.py`) only carry an ordered list of blocks. Execution (`WorkflowService._execute_workflow_blocks`) loops through this list and stops on failure unless `continue_on_failure` is set.\n- Nesting today is restricted to `ForLoopBlock`; `get_all_blocks` (in `skyvern/forge/sdk/workflow/models/block.py`) only knows how to flatten for-loops.\n- Blocks log execution by inserting a `WorkflowRunBlockModel` row with a parent pointer when invoked via `execute_safe`. Conditional branching must keep leveraging that machinery so timelines, auditing, and script regeneration keep working.\n- Script generation (`skyvern/core/script_generations/generate_script.py` and `skyvern/services/script_service.py`) mirrors the flat list; adding a new block requires updates for both AI and cached-code paths.\n\n## Detailed Design\n\n### 1. Workflow Graph Representation\n\n- Add `WorkflowGraph` helper(s) under `skyvern/forge/sdk/workflow/` that:\n  - Build a node map (`label -> BlockTypeVar`) and adjacency list by walking `workflow_definition.blocks`.\n  - Default edges follow the existing list order; nested containers (for-loops, conditional branches) contribute their own subgraphs but still reconnect to the parent’s “next” node.\n  - Provide APIs such as `graph.iterate_from(label)` and `graph.find_branch_entry(parent_label, branch_label)` to share between the executor, script generation, and future analytics.\n- `get_all_blocks` should consult the graph builder instead of manually checking only for for-loops so conditional-branch blocks participate in bulk operations (validation, caching, search).\n- Extend `WorkflowDefinition.validate()` to ensure block labels remain unique across the entire graph (top-level + nested branches) and to detect impossible graphs (e.g., missing branch entry labels).\n\n### 2. Block Modeling & Schemas\n\n- Introduce `BlockType.CONDITIONAL` in `skyvern/schemas/workflows.py` and `BlockType` enum consumers (OpenAPI/Fern definitions, `skyvern-ts`, Python client types, etc.).\n- Define supporting schemas:\n  - `ConditionExpressionYAML`: `{ expression: str, language: Literal[\"jinja\"], parameter_keys: list[str] = [], description: str | None }`.\n  - `ConditionalBranchYAML`: `{ branch_label: str, predicate: ConditionExpressionYAML | None, blocks: list[BLOCK_YAML_TYPES] }`. `predicate=None` represents the `else` branch.\n  - `ConditionalBlockYAML`: inherits `BlockYAML`, adds `branches: list[ConditionalBranchYAML]` and optional `max_evaluations` guard (default len(branches)).\n- Mirror these models in `skyvern/forge/sdk/workflow/models/block.py` as `ConditionExpression`, `ConditionalBranch`, and `ConditionalBlock` (extending `Block`).\n  - `ConditionalBlock.get_all_parameters()` should union parameters referenced by `parameter_keys` and all nested branch block parameters.\n  - Each branch stores a generated `branch_workflow_run_block_id` parent pointer when executing descendant blocks so DB rows stay threaded to the conditional container.\n- Default output payload of a conditional block: `{ \"selected_branch\": str, \"condition_results\": list[ {\"branch_label\": str, \"expression\": str, \"result\": bool } ], \"branch_outputs\": dict[label, Any] }`. This becomes the recorded output parameter value for the block’s `output_parameter` to keep downstream steps templatable.\n\n### 3. Condition Evaluation Semantics\n\n- Use the existing `jinja_sandbox_env` with StrictUndefined when `settings.WORKFLOW_TEMPLATING_STRICTNESS == \"strict\"` (same as other templated fields) to render each predicate. The template context exposes:\n  - `params`: workflow/run parameters from `WorkflowRunContext.values`.\n  - `outputs`: shorthand for `{label: workflow_run_context.values[f\"{label}_output\"]}` for completed blocks.\n  - `metadata`: per-block metadata recorded via `WorkflowRunContext.blocks_metadata`.\n  - Basic runtime data (`workflow`, `organization`, timestamps) supplied by `WorkflowRunContext` so conditions can reference e.g. `params.customer_tier` or `outputs.validate_invoice.success`.\n- Evaluated predicate must coerce to `bool`; invalid expressions mark the conditional block as failed (unless `continue_on_failure` is set) and record the exception message in `condition_results`.\n- Support optional `default_value` for predicates to treat missing data as false without raising. (Implementation detail: when StrictUndefined triggers, catch and treat as `False` with warning.)\n\n### 4. Execution Runtime Updates\n\n- In `ConditionalBlock.execute()`:\n  1. Persist `condition_results` in the conditional block’s `WorkflowRunBlockModel` row as predicates are evaluated.\n  2. Select the first branch whose predicate resolves `True`. If none matches, fall back to the branch with `predicate=None`. If still no branch, mark the block as terminated with failure reason “No matching branch”.\n  3. Invoke a new helper (`execute_nested_blocks(blocks, parent_block_id, workflow_run_id, ...)`) that mirrors `_execute_workflow_blocks` but accepts a list of blocks and passes `parent_workflow_run_block_id` so nested rows are linked.\n  4. Aggregate resulting `BlockResult`s to determine the conditional block’s own `BlockStatus` (failed if any nested block failed without `continue_on_failure`, canceled if nested path canceled, etc.).\n  5. Update the workflow context with `selected_branch` and the nested outputs (namespaced under each block label) before returning.\n- Update `WorkflowService._execute_workflow_blocks` to:\n  - Build a `WorkflowGraph` once, then iterate nodes using the graph instead of relying only on list indices. For this release we still execute depth-first/sequentially, but the graph representation ensures branch nodes don’t get double-executed.\n  - Respect `block_labels` filters by interpreting them as graph node labels (a filtered run only executes the specified nodes and their ancestors needed to fulfill dependencies).\n  - Ensure cancellation/timeout propagation works when failures happen inside a nested branch (surface the exception back to the parent conditional block and halt if necessary).\n- Update `script_service`, `workflow_script_service`, and `skyvern/core/script_generations/generate_script.py` to:\n  - Generate idiomatic Python `if/elif/else` structures that wrap the nested blocks when `generate_script` is enabled.\n  - Include serialized predicate metadata in `ScriptBlockSource` so cached code can short-circuit and skip AI execution when the condition is false (by simply not invoking the branch blocks).\n  - Ensure cached block lookups include the conditional block label so branching decisions are reproducible between AI and code paths.\n\n### 5. Persistence & Observability\n\n- Extend `WorkflowRunBlockModel` (`skyvern/forge/sdk/db/models.py`) with:\n  - `condition_expression` (JSON list storing each predicate string and rendered context),\n  - `condition_result` (JSON list of booleans + error messages),\n  - `selected_branch_label` (string),\n  - `branch_metadata` (JSON for aggregate outputs if needed for quick lookups).\n  Alembic migration adds nullable columns with backfills defaulting to `NULL`.\n- Update `WorkflowRunBlock` API schema (`skyvern/forge/sdk/schemas/workflow_runs.py` and client mirrors) to include the new fields so UI and logs can show branch choices.\n- Emit structured logs + metrics (e.g., `conditional_branch_selected`) with workflow/run IDs, branch label, evaluation duration, and predicate hash to monitor adoption.\n\n### 6. API, Client, and Validation Changes\n\n- Update `WorkflowCreateYAMLRequest` / `WorkflowDefinitionYAML` to allow the new block, and ensure YAML import/export (including `docs/examples/workflows/*.yaml`) mention conditional branching.\n- Update REST + Fern/TS/Python clients to serialize/deserialize `ConditionalBlock` and nested branch structures. Regenerate SDKs as part of rollout.\n- Update server-side validation:\n  - Branch list must contain at least one predicate or an else branch.\n  - Branch labels must be unique within the conditional block.\n  - Each branch must contain at least one block (or explicitly opt into allowing empty, which would be a no-op path).\n  - Detect cycles when users point branches back to ancestor nodes once DAG support is expanded.\n\n### 7. Frontend & Authoring Experience\n\n- Add a “Conditional” block type to the workflow editor palette in `skyvern-frontend`:\n  - Provide inputs for branch label, predicate expression (with syntax highlighting, auto-suggest for parameter keys/output labels), and helper text referencing the runtime context keys.\n  - Allow adding/removing/reordering branches and toggling a default `else` branch.\n  - Within each branch, reuse the existing block composer to create nested sequences (drag blocks inside the branch panel). Persisted JSON/YAML matches backend schema exactly.\n  - Surface branch telemetry in the run viewer (e.g., show which branch executed, predicate value, nested block statuses) using new fields from `WorkflowRunBlock` API responses.\n- Update docs (`docs/` and README) with examples of using conditional branching and referencing outputs in predicates.\n\n### 8. Migration & Rollout\n\n- Feature-flag conditional blocks (e.g., `ENABLE_CONDITIONAL_BLOCKS`) at the API layer so we can limit to internal orgs until the UI ships.\n- No existing workflows require migration because the new block is additive. Ensure exporters skip the field for legacy definitions.\n- Backfill graph metadata lazily: when a workflow lacking explicit graph info loads, auto-infer edges from list order (maintaining backward compatibility) and cache the computed graph representation alongside the workflow record for faster execution.\n\n### 9. Caching & Script Generation Guarantees\n\n- **Path-agnostic cache behavior:** Nested blocks under a conditional branch must compute cache keys using the same inputs as sequential execution (workflow parameters, workflow-level cache key, optional block-level templates). Branch selection cannot alter cache eligibility—if a block was cacheable before branching, it remains so regardless of which branch executed it.\n- **Branch lineage metadata:** When a conditional block chooses a branch, persist `selected_branch_label` plus an ordered `branch_path` (e.g., `['conditional_shipping', 'international']`) on the workflow context so script generation and cache lookups can include that lineage when needed. This prevents collisions if future DAG work allows the same downstream block to be reachable from multiple parents.\n- **Script generation parity:** `_build_block_fn`, `generate_workflow_script_python_code`, and `_regenerate_script_block_after_ai_fallback` must emit identical decorators/cache annotations for branch-executed blocks as for top-level ones. Conditional scaffolding (the generated `if/elif/else`) simply guards the call site; the wrapped function names, cache identifiers, and stored signatures stay unchanged so cached code can be reused by any branch that reaches that block.\n- **Sequential parity in generated code:** Maintain the existing per-block decorator pattern (`@skyvern.block(cache_key=f\"{workflow_cache_key}:{block_label}\")`) so each block keeps its stable cached function regardless of branch membership. Conditional logic in the generated script only wraps the invocation site (e.g., Python `if/elif/else`), and the runtime still records `selected_branch` so `WorkflowRunBlock` rows align with the cached function outputs.\n- **Cache invalidation hooks:** When conditional or nested block execution fails and falls back to AI, reuse the same cache invalidation path (e.g., `blocks_to_update` bookkeeping) and ensure branch context is logged to help debug cache misses.\n- **Testing:** Add regression tests that execute the same block via two different branches with caching enabled to confirm cached outputs/scripts are reused and no duplicate artifacts are created.\n\n## Testing Strategy\n\n- Unit tests for `ConditionalBlock` covering predicate evaluation (success, strict undefined, invalid template), branch selection order, else fallback, and nested block aggregation.\n- Integration tests in `tests/workflow` that create workflows with conditional branches, run them with parameters triggering each branch, and verify only the expected blocks execute (including script-generated runs).\n- Regression tests for YAML import/export and client SDK serialization to ensure nested branch structures survive round-trips.\n- Frontend component + Cypress/e2e tests that author a conditional block, save, reload, and observe run history highlighting the chosen branch.\n- Load tests to ensure predicate evaluation and nested execution do not materially regress workflow runtimes.\n\n## Open Questions\n\n1. Should predicates support additional languages besides Jinja (e.g., JSONPath comparisons) to make simple equality checks easier to author?\n2. Do we need UI/runtime safeguards against overly deep nesting (conditional inside loop inside conditional) until we harden stack-safety and timeout heuristics?\n3. How should we expose branch-level metrics to customers (e.g., % of runs taking each path) and should that live in the analytics service or workflow run timeline API?\n4. For future DAG features, do we allow branches to converge on shared downstream nodes, and if so how do we de-duplicate block execution when both paths would otherwise reach the same block?", "priority": 1, "archived": 0, "archived_at": null, "created_at": "2025-11-11 05:19:57", "updated_at": "2025-11-11 05:25:10", "parent_id": null, "parent_uuid": null, "relationships": [], "tags": [""]}
